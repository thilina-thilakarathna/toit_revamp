{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f97ff151",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, roc_curve,\n",
    "    precision_score, recall_score, f1_score, accuracy_score,\n",
    "    confusion_matrix\n",
    ")\n",
    "\n",
    "def _bin_true(y, positive='T'):\n",
    "    # Map labels -> {0,1}\n",
    "    y = pd.Series(y).astype(str)\n",
    "    return (y == positive).astype(int).to_numpy()\n",
    "\n",
    "def _bin_pred(y, positive='T'):\n",
    "    y = pd.Series(y).astype(str)\n",
    "    return (y == positive).astype(int).to_numpy()\n",
    "\n",
    "def _safe_auc(y_true_bin, scores):\n",
    "    scores = np.asarray(scores, dtype=float)\n",
    "    mask = np.isfinite(scores)\n",
    "    y = y_true_bin[mask]\n",
    "    s = scores[mask]\n",
    "    # AUC requires both classes present\n",
    "    if len(np.unique(y)) < 2:\n",
    "        return np.nan\n",
    "    return float(roc_auc_score(y, s))\n",
    "\n",
    "def eval_stage(df, y_true_col, pred_col, score_col=None, positive='T', name=''):\n",
    "    d = df.copy()\n",
    "\n",
    "    y_true = _bin_true(d[y_true_col], positive=positive)\n",
    "    y_pred = _bin_pred(d[pred_col], positive=positive)\n",
    "\n",
    "    out = {\n",
    "        \"stage\": name,\n",
    "        \"n\": int(len(d)),\n",
    "        \"pos_rate_true\": float(y_true.mean()) if len(y_true) else np.nan,\n",
    "        \"accuracy\": float(accuracy_score(y_true, y_pred)) if len(y_true) else np.nan,\n",
    "        \"precision\": float(precision_score(y_true, y_pred, zero_division=0)) if len(y_true) else np.nan,\n",
    "        \"recall\": float(recall_score(y_true, y_pred, zero_division=0)) if len(y_true) else np.nan,\n",
    "        \"f1\": float(f1_score(y_true, y_pred, zero_division=0)) if len(y_true) else np.nan,\n",
    "        \"cm\": confusion_matrix(y_true, y_pred).tolist() if len(y_true) else None,\n",
    "    }\n",
    "\n",
    "    if score_col is not None:\n",
    "        out[\"auc\"] = _safe_auc(y_true, d[score_col].to_numpy())\n",
    "    else:\n",
    "        out[\"auc\"] = np.nan\n",
    "\n",
    "    return out\n",
    "\n",
    "def evaluate_detection(df_out, true_col='true_label'):\n",
    "    # -----------------------------\n",
    "    # Stage A: origin mapping\n",
    "    # pred: label_test == 'LT'\n",
    "    # score: score_R\n",
    "    # -----------------------------\n",
    "    stageA = df_out[df_out['origin'] == 'R'].copy()\n",
    "    stageA[\"predA\"] = np.where(stageA.get(\"label_test\", \"BMP\") == \"LT\", \"T\", \"C\")\n",
    "    resA = eval_stage(\n",
    "        stageA,\n",
    "        y_true_col=true_col,\n",
    "        pred_col=\"predA\",\n",
    "        score_col=\"score_R\",\n",
    "        positive=\"T\",\n",
    "        name=\"A) Origin mapping (R vs remote)\"\n",
    "    )\n",
    "\n",
    "    # -----------------------------\n",
    "    # Stage B: BMP relabeling\n",
    "    # only those that were BMP after stage A\n",
    "    # pred: final label (T/C)\n",
    "    # score: score (should be knn score_b you computed)\n",
    "    # -----------------------------\n",
    "    stageB = df_out[(df_out['origin'] == 'R') & (df_out.get('label_test', '') == 'BMP')].copy()\n",
    "    # If you want to evaluate only records your stage B *actually* decided on:\n",
    "    stageB = stageB[stageB['label'].isin(['C', 'T'])].copy()\n",
    "\n",
    "    resB = eval_stage(\n",
    "        stageB,\n",
    "        y_true_col=true_col,\n",
    "        pred_col=\"label\",\n",
    "        score_col=\"score\",   # make sure score got copied back into df_out\n",
    "        positive=\"T\",\n",
    "        name=\"B) BMP→{C,T} (anchor kNN)\"\n",
    "    )\n",
    "\n",
    "    # -----------------------------\n",
    "    # Stage C: Generated scoring\n",
    "    # pred: final label\n",
    "    # score: score_G (or score)\n",
    "    # -----------------------------\n",
    "    stageC = df_out[df_out['origin'] == 'G'].copy()\n",
    "    stageC = stageC[stageC['label'].isin(['C', 'T'])].copy()\n",
    "\n",
    "    resC = eval_stage(\n",
    "        stageC,\n",
    "        y_true_col=true_col,\n",
    "        pred_col=\"label\",\n",
    "        score_col=\"score_G\",  # or \"score\"\n",
    "        positive=\"T\",\n",
    "        name=\"C) Generated detection (3-part score)\"\n",
    "    )\n",
    "\n",
    "    # -----------------------------\n",
    "    # End-to-end (optional)\n",
    "    # -----------------------------\n",
    "    end2end = df_out[df_out['label'].isin(['C','T'])].copy()\n",
    "    resE = eval_stage(\n",
    "        end2end,\n",
    "        y_true_col=true_col,\n",
    "        pred_col=\"label\",\n",
    "        score_col=\"score\",\n",
    "        positive=\"T\",\n",
    "        name=\"Overall) End-to-end\"\n",
    "    )\n",
    "\n",
    "    return pd.DataFrame([resA, resB, resC, resE])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94c0615f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded existing evaluation data from CSV.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from evaluations.evaluation_data.evaluation_data import EvaluationData\n",
    "from tampering.tampering import Tampering\n",
    "from timf.timf import TIMF\n",
    "from data_service.data_service import DataService\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\n",
    "\n",
    "\n",
    "\n",
    "evaluation_data = EvaluationData()\n",
    "tampering = Tampering()    \n",
    "data_service = DataService()\n",
    "timf = TIMF(data_service)\n",
    "data = evaluation_data.get_data()\n",
    "\n",
    "\n",
    "def _haversine_km(lat1, lon1, lat2, lon2):\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    return 6371.0 * c\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\n",
    "\n",
    "def compute_metrics_from_labels(y_true_labels, y_pred_labels):\n",
    "    \"\"\"\n",
    "    Inputs are arrays/Series with values 'T' or 'C'\n",
    "    \"\"\"\n",
    "    y_true = (pd.Series(y_true_labels) == 'T').astype(int)\n",
    "    y_pred = (pd.Series(y_pred_labels) == 'T').astype(int)\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "    try:\n",
    "        auc = roc_auc_score(y_true, y_pred)\n",
    "    except ValueError:\n",
    "        auc = None\n",
    "\n",
    "    return accuracy, precision, recall, auc\n",
    "\n",
    "def _to_bin_labels(y):\n",
    "    \"\"\"Map labels to 0/1 where 'T' is positive (1), everything else is 0.\"\"\"\n",
    "    return np.array([1 if str(v) == 'T' else 0 for v in y], dtype=int)\n",
    "\n",
    "def safe_auc(y_true_labels, y_scores):\n",
    "    \"\"\"\n",
    "    Robust AUC:\n",
    "    - returns np.nan if empty or only one class present\n",
    "    - expects y_true_labels like ['T','C',...] and y_scores numeric\n",
    "    \"\"\"\n",
    "    if y_true_labels is None or y_scores is None:\n",
    "        return np.nan\n",
    "    if len(y_true_labels) == 0 or len(y_scores) == 0:\n",
    "        return np.nan\n",
    "\n",
    "    y_true_bin = _to_bin_labels(y_true_labels)\n",
    "    y_scores = np.asarray(y_scores, dtype=float)\n",
    "\n",
    "    # lengths must match\n",
    "    if y_true_bin.shape[0] != y_scores.shape[0]:\n",
    "        raise ValueError(f\"AUC length mismatch: y_true={len(y_true_bin)} vs y_score={len(y_scores)}\")\n",
    "\n",
    "    # AUC needs both classes\n",
    "    if np.unique(y_true_bin).size < 2:\n",
    "        return np.nan\n",
    "\n",
    "    return float(roc_auc_score(y_true_bin, y_scores))\n",
    "\n",
    "def compute_metrics_from_labels(y_true_labels, y_pred_labels):\n",
    "    \"\"\"\n",
    "    Your existing label-based metrics (Accuracy / Precision / Recall).\n",
    "    Assumes positive class is 'T'.\n",
    "    \"\"\"\n",
    "    y_true_bin = _to_bin_labels(y_true_labels)\n",
    "    y_pred_bin = _to_bin_labels(y_pred_labels)\n",
    "\n",
    "    acc = float(accuracy_score(y_true_bin, y_pred_bin))\n",
    "    prec = float(precision_score(y_true_bin, y_pred_bin, zero_division=0))\n",
    "    rec = float(recall_score(y_true_bin, y_pred_bin, zero_division=0))\n",
    "    return acc, prec, rec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a88367e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tampering_percentages = list(range(10, 100, 40))\n",
    "\n",
    "tampering_types = [\"S\"]  # Naive, Knowledgeable, Sophisticated\n",
    "\n",
    "availability = 1\n",
    "repliocation_mic = 5\n",
    "# tampering_types = [\"N\"]\n",
    "        \n",
    "results = []\n",
    "\n",
    "results_metrics = []\n",
    "\n",
    "\n",
    "\n",
    "for tampering_type in tampering_types:\n",
    "    for tampering_percentage in tampering_percentages:\n",
    "\n",
    "        print(f\"Experiment: Tampering Type={tampering_type}, Percentage={tampering_percentage}%\")\n",
    "        cpunt1=0\n",
    "        cpunt2=0\n",
    "\n",
    "        # for assessing_mic in data['gen_microcell'].unique():\n",
    "        microcells = np.random.choice(\n",
    "            data['gen_microcell'].unique(),\n",
    "            size=5,\n",
    "            replace=False\n",
    "        )\n",
    "\n",
    "        for assessing_mic in microcells:\n",
    "            # print(assessing_mic)\n",
    "            full_df = data.copy()\n",
    "            df_microcell = data[data['gen_microcell'] == assessing_mic].copy()\n",
    "            remaining_df = data[data['gen_microcell'] != assessing_mic].copy()\n",
    "\n",
    "            # -------- Remote tampering (BMA) --------\n",
    "            bma_tampered_df = tampering.bma_tampering(\n",
    "                remaining_df.reset_index(drop=True),\n",
    "                tampering_percentage,\n",
    "                tampering_type\n",
    "            )\n",
    "            remote_data = bma_tampered_df.copy()\n",
    "\n",
    "            # remote_data = data[data['gen_microcell'] != assessing_mic].reset_index(drop=True)\n",
    "\n",
    "            # -------- Replication logic --------\n",
    "            microcell_coords = full_df.groupby('gen_microcell')[['latitude', 'longitude']].first().reset_index()\n",
    "            current_coords = microcell_coords[microcell_coords['gen_microcell'] == assessing_mic]\n",
    "\n",
    "            if not current_coords.empty:\n",
    "                lat1 = current_coords['latitude'].values[0]\n",
    "                lon1 = current_coords['longitude'].values[0]\n",
    "\n",
    "                df_microcell_part = df_microcell.copy()\n",
    "                df_microcell_part.loc[:, 'currect_microcell'] = assessing_mic\n",
    "\n",
    "                replicated_parts = [df_microcell_part]\n",
    "\n",
    "                for provider_id in df_microcell['providerid'].unique():\n",
    "\n",
    "                    provider_remote = remote_data[remote_data['providerid'] == provider_id]\n",
    "                    candidate_microcells = []\n",
    "\n",
    "                    for _, row in microcell_coords.iterrows():\n",
    "                        if row['gen_microcell'] == assessing_mic:\n",
    "                            continue\n",
    "                        if (provider_remote['gen_microcell'] == row['gen_microcell']).any():\n",
    "                            dist = _haversine_km(lat1, lon1, row['latitude'], row['longitude'])\n",
    "                            candidate_microcells.append((row['gen_microcell'], dist))\n",
    "\n",
    "                    candidate_microcells.sort(key=lambda x: x[1])\n",
    "                    nearby_microcells = [m for m, _ in candidate_microcells[:repliocation_mic]]\n",
    "\n",
    "                    if nearby_microcells:\n",
    "                        df_remote = remote_data[\n",
    "                            (remote_data['providerid'] == provider_id) &\n",
    "                            (remote_data['gen_microcell'].isin(nearby_microcells))\n",
    "                        ].drop_duplicates(subset='serviceid')\n",
    "\n",
    "                        if not df_remote.empty:\n",
    "                            df_remote['origin'] = 'R'\n",
    "                            df_remote['currect_microcell'] = assessing_mic\n",
    "                            replicated_parts.append(df_remote)\n",
    "\n",
    "                df_microcell_replicated = pd.concat(replicated_parts, ignore_index=True)\n",
    "            else:\n",
    "                df_microcell_replicated = df_microcell.copy()\n",
    "\n",
    "            # -------- Local tampering (SPA) --------\n",
    "            spa_tampered_df = tampering.spa_tampering(\n",
    "                df_microcell_replicated,\n",
    "                type=tampering_type\n",
    "            )\n",
    "\n",
    "            # -------- Set TIMF data --------\n",
    "            data_service.set_local_data(spa_tampered_df.copy())\n",
    "\n",
    "            # --- Full remote pool (all other microcells) ---\n",
    "            # remote_data_set = full_df[full_df['gen_microcell'] != assessing_mic].copy()\n",
    "\n",
    "            # remote_data_set = data[data['gen_microcell'] != assessing_mic].copy()\n",
    "\n",
    "            remote_data_set = bma_tampered_df.copy()\n",
    "\n",
    "            rng = np.random.default_rng(42)  # fixed seed for reproducibility\n",
    "            uni_mic = remote_data_set['gen_microcell'].unique()\n",
    "\n",
    "            selected_mics = rng.choice(\n",
    "                uni_mic,\n",
    "                size=max(1, int(len(uni_mic) * 1)),\n",
    "                replace=False\n",
    "            )\n",
    "\n",
    "            remote_available = remote_data_set[remote_data_set['gen_microcell'].isin(selected_mics)].copy()\n",
    "            data_service.set_remote_data(remote_available)\n",
    "\n",
    "            correct_available = data[\n",
    "                (data['gen_microcell'] != assessing_mic) &\n",
    "                (data['gen_microcell'].isin(selected_mics))\n",
    "            ].copy()\n",
    "\n",
    "            data_service.set_correct_data(correct_available)\n",
    "            # data_service.set_correct_data(data[data['gen_microcell'] != assessing_mic].copy())\n",
    "\n",
    "            # =====================================================\n",
    "            # Collect metrics per microcell across all providers\n",
    "            # =====================================================\n",
    "            tda_true, tda_pred = [], []\n",
    "            tda_time_entries = []\n",
    "\n",
    "            # ---- NEW: for AUC we need continuous scores\n",
    "            # Overall (optional)\n",
    "            tda_true_all, tda_score_all = [], []\n",
    "            # Part 1 (R)\n",
    "            tda_true_R, tda_score_R = [], []\n",
    "            # Part 2 (G)\n",
    "            tda_true_G, tda_score_G = [], []\n",
    "\n",
    "            baseline_true = {'IsolationForest': [], 'LOF': [], 'OCSVM': []}\n",
    "            baseline_pred = {'IsolationForest': [], 'LOF': [], 'OCSVM': []}\n",
    "            baseline_time_entries = {'IsolationForest': [], 'LOF': [], 'OCSVM': []}\n",
    "\n",
    "            # If your baselines also expose scores later, you can add baseline_score lists too.\n",
    "\n",
    "            for provider in df_microcell['providerid'].unique():\n",
    "                df_p = spa_tampered_df[spa_tampered_df['providerid'] == provider]\n",
    "\n",
    "                n_gen = (df_p['origin'] == 'G').sum()\n",
    "                n_rec = (df_p['origin'] == 'R').sum()\n",
    "\n",
    "                if n_gen >= 5 and n_rec >= 5:\n",
    "                    cpunt1=cpunt1+1\n",
    "                # print(provider)\n",
    "\n",
    "                # ---------- TDA ----------\n",
    "                # IMPORTANT: df_tda must contain: origin, true_label, label, and score_R/score_G/score\n",
    "                    _, df_tda, time_df = timf.trust_assessment(provider, assessing_mic)\n",
    "                else:\n",
    "                    cpunt2=cpunt2+1\n",
    "                    print(\"Skip\")\n",
    "        print(cpunt1,cpunt2)\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "            \n",
    "    \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f65d0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5e5d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac3100e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(results_metrics)\n",
    "\n",
    "metrics_table = df[\n",
    "    [\n",
    "        \"tampering_type\",\n",
    "        \"tampering_percentage\",\n",
    "        \"microcell\",\n",
    "        \"method\",\n",
    "        \"accuracy\",\n",
    "        \"precision\",\n",
    "        \"recall\",\n",
    "        \"AUC_all\",\n",
    "        \"AUC_R\",\n",
    "        \"AUC_G\",\n",
    "    ]\n",
    "].copy()\n",
    "\n",
    "metrics_table = metrics_table.sort_values(\n",
    "    by=[\"tampering_type\", \"tampering_percentage\", \"method\"]\n",
    ").reset_index(drop=True)\n",
    "\n",
    "metrics_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b427001e",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_metrics = (\n",
    "    metrics_table\n",
    "    .groupby(\n",
    "        [\"tampering_type\", \"tampering_percentage\", \"method\"],\n",
    "        as_index=False\n",
    "    )\n",
    "    .mean(numeric_only=True)\n",
    ")\n",
    "\n",
    "avg_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e273da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_metrics.to_csv('paper_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fb5cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "\n",
    "# Pretty names for methods (legend)\n",
    "method_labels = {\n",
    "    \n",
    "    \"TDA\": \"TDA (our approch)\",\n",
    "    \"IsolationForest\": \"IFUM\",\n",
    "    \"LOF\": \"LOFM\",\n",
    "    \"OCSVM\": \"OCSVM\",\n",
    "    \"TIMF\": \"TIMF\"\n",
    "}\n",
    "\n",
    "# Pretty names for tampering types (title)\n",
    "tampering_labels = {\n",
    "    \"N\": \"Naïve Attack\",\n",
    "    \"K\": \"Knowledge-based Attack\",\n",
    "    \"S\": \"Sophisticated Attack\"\n",
    "}\n",
    "\n",
    "# Pretty names for metrics (subplot titles / y-axis)\n",
    "metric_labels = {\n",
    "    \"accuracy\": \"(A) Accuracy\",\n",
    "    \"precision\": \"(B) Precision\",\n",
    "    \"recall\": \"(R) Recall\"\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Prepare the DataFrame\n",
    "metrics_df = pd.DataFrame(results_metrics).dropna(\n",
    "    subset=['accuracy', 'precision', 'recall']\n",
    ")\n",
    "\n",
    "# Compute mean metrics\n",
    "summary = (\n",
    "    metrics_df\n",
    "    .groupby(['tampering_type', 'tampering_percentage', 'method'], as_index=False)\n",
    "    [['accuracy', 'precision', 'recall']]\n",
    "    .mean()\n",
    ")\n",
    "\n",
    "tampering_types = [\"N\", \"K\", \"S\"]\n",
    "metric_names = ['accuracy', 'precision', 'recall']\n",
    "\n",
    "# Marker and line-style cycles (color-blind friendly)\n",
    "markers = cycle(['o', 's', '^', 'D',])\n",
    "linestyles = cycle(['-', '--', '-.', ':'])\n",
    "\n",
    "method_colors = {\n",
    "    \"TDA\": \"tab:green\",          # proposed method\n",
    "    \"IsolationForest\": \"tab:orange\",\n",
    "    \"LOF\": \"tab:blue\",\n",
    "    \"OCSVM\": \"tab:red\",\n",
    "    \"TIMF\": \"tab:purple\"\n",
    "}\n",
    "\n",
    "\n",
    "# Loop over each tampering type\n",
    "for t_type in tampering_types:\n",
    "    subset_type = summary[summary['tampering_type'] == t_type]\n",
    "    methods = subset_type['method'].unique()\n",
    "\n",
    "    # Assign a unique (marker, linestyle) per method\n",
    "    style_map = {\n",
    "        method: (next(markers), next(linestyles))\n",
    "        for method in methods\n",
    "    }\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5), sharex=True)\n",
    "    # fig.suptitle(f'Metrics for Tampering Type {t_type}', fontsize=16)\n",
    "\n",
    "    for idx, metric in enumerate(metric_names):\n",
    "        ax = axes[idx]\n",
    "\n",
    "        for method in methods:\n",
    "            subset_method = subset_type[subset_type['method'] == method]\n",
    "            marker, linestyle = style_map[method]\n",
    "\n",
    "            ax.plot(\n",
    "                subset_method['tampering_percentage'],\n",
    "                subset_method[metric],\n",
    "                marker=marker,\n",
    "                linestyle=linestyle,\n",
    "                linewidth=2,\n",
    "                markersize=6,\n",
    "                label=method_labels.get(method, method),\n",
    "                 color=method_colors.get(method, None)  # <-- force blue for TDA\n",
    "            )\n",
    "\n",
    "        ax.set_title(metric.capitalize())\n",
    "        ax.set_xlabel('Tampering super-provider percentage (%)')\n",
    "        ax.set_ylabel(metric.capitalize())\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "    axes[0].legend(title='Method', frameon=True)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f9d26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(results_metrics)\n",
    "\n",
    "# focus on TDA only\n",
    "df_tda = df[df['method'] == 'TDA']\n",
    "\n",
    "# average across microcells\n",
    "agg = df_tda.groupby('tampering_percentage').agg({\n",
    "    'AUC_R': 'mean',\n",
    "    'AUC_G': 'mean',\n",
    "    'AUC_all': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(agg['tampering_percentage'], agg['AUC_R'], marker='o', label='TDA (AUC_R)')\n",
    "plt.plot(agg['tampering_percentage'], agg['AUC_G'], marker='s', label='TDA (AUC_G)')\n",
    "\n",
    "# optional\n",
    "if 'AUC_all' in agg:\n",
    "    plt.plot(agg['tampering_percentage'], agg['AUC_all'], marker='^', linestyle='--', label='TDA (AUC_all)')\n",
    "\n",
    "plt.xlabel('tampering super provider percentage (%)')\n",
    "plt.ylabel('AUC')\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0be98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d1ac68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(results_metrics)\n",
    "\n",
    "# focus on TDA only\n",
    "df_tda = df[df['method'] == 'TDA']\n",
    "\n",
    "tampering_types = ['N', 'K', 'S']\n",
    "titles = {\n",
    "    'N': 'Scenario 1: Naive tampering',\n",
    "    'K': 'Scenario 2: Knowledgeable tampering',\n",
    "    'S': 'Scenario 3: Sophisticated tampering'\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4), sharey=True)\n",
    "\n",
    "for ax, ttype in zip(axes, tampering_types):\n",
    "    df_t = df_tda[df_tda['tampering_type'] == ttype].copy()\n",
    "\n",
    "    # ---- polarity correction ONLY for S ----\n",
    "    if ttype == 'S':\n",
    "        df_t['AUC_G_corr'] = df_t['AUC_G'].apply(lambda x: max(x, 1 - x))\n",
    "        df_t['AUC_all_corr'] = df_t['AUC_all'].apply(lambda x: max(x, 1 - x))\n",
    "    else:\n",
    "        df_t['AUC_G_corr'] = df_t['AUC_G']\n",
    "        df_t['AUC_all_corr'] = df_t['AUC_all']\n",
    "\n",
    "    agg = df_t.groupby('tampering_percentage').agg({\n",
    "        'AUC_R': 'mean',\n",
    "        'AUC_G_corr': 'mean',\n",
    "        'AUC_all_corr': 'mean'\n",
    "    }).reset_index()\n",
    "\n",
    "    ax.plot(agg['tampering_percentage'], agg['AUC_R'],\n",
    "            marker='o', label='AUC_R')\n",
    "    ax.plot(agg['tampering_percentage'], agg['AUC_G_corr'],\n",
    "            marker='s', label='AUC_G')\n",
    "\n",
    "\n",
    "    ax.set_title(titles[ttype])\n",
    "    ax.set_xlabel('tampering super provider percentage (%)')\n",
    "    ax.grid(True)\n",
    "\n",
    "# shared Y label\n",
    "axes[0].set_ylabel('AUC')\n",
    "axes[0].set_ylim(0, 1.0)\n",
    "\n",
    "# single legend for the whole figure\n",
    "handles, labels = axes[0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='upper center', ncol=3, frameon=False)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.9])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe1fd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(results_metrics)\n",
    "\n",
    "# focus on TDA only\n",
    "df_tda = df[df['method'] == 'TDA']\n",
    "\n",
    "tampering_types = ['N', 'K', 'S']\n",
    "\n",
    "for ttype in tampering_types:\n",
    "    df_t = df_tda[df_tda['tampering_type'] == ttype]\n",
    "\n",
    "    table = (\n",
    "        df_t\n",
    "        .groupby('tampering_percentage')[['AUC_R', 'AUC_G']]\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "        .round(4)\n",
    "    )\n",
    "\n",
    "    print(f\"\\n=== TDA AUC Table – {ttype} Tampering ===\")\n",
    "    print(table.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ac01a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef2cd55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7797df04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d489487a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e3b600",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b369979",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4026d09e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2aef4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(results_metrics)\n",
    "\n",
    "avg_df = (\n",
    "    df\n",
    "    .groupby(['method', 'tampering_type','tampering_percentage'])[['accuracy', 'precision', 'recall']]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "print(avg_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd218d55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c971a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "tampering_percentages = list(range(10, 100, 10))\n",
    "\n",
    "# tampering_types = [\"N\", \"K\", \"S\"]  # Naive, Knowledgeable, Sophisticated\n",
    "tampering_types = [\"N\"]\n",
    "        \n",
    "results = []\n",
    "\n",
    "results_metrics = []\n",
    "\n",
    "for tampering_type in tampering_types:\n",
    "    for tampering_percentage in tampering_percentages:\n",
    "\n",
    "        print(f\"Experiment: Tampering Type={tampering_type}, Percentage={tampering_percentage}%\")\n",
    "\n",
    "        for assessing_mic in data['gen_microcell'].unique():\n",
    "            full_df = data.copy()\n",
    "            df_microcell = data[data['gen_microcell'] == assessing_mic].copy()\n",
    "            remaining_df = data[data['gen_microcell'] != assessing_mic].copy()\n",
    "\n",
    "            # -------- Remote tampering (BMA) --------\n",
    "            bma_tampered_df = tampering.bma_tampering(\n",
    "                remaining_df.reset_index(drop=True),\n",
    "                tampering_percentage,\n",
    "                tampering_type\n",
    "            )\n",
    "            remote_data = bma_tampered_df\n",
    "\n",
    "            # remote_data = data[data['gen_microcell'] != assessing_mic].reset_index(drop=True)\n",
    "\n",
    "            # -------- Replication logic --------\n",
    "            microcell_coords = full_df.groupby('gen_microcell')[['latitude', 'longitude']].first().reset_index()\n",
    "            current_coords = microcell_coords[microcell_coords['gen_microcell'] == assessing_mic]\n",
    "\n",
    "            if not current_coords.empty:\n",
    "                lat1 = current_coords['latitude'].values[0]\n",
    "                lon1 = current_coords['longitude'].values[0]\n",
    "\n",
    "                df_microcell_part = df_microcell.copy()\n",
    "                df_microcell_part.loc[:, 'currect_microcell'] = assessing_mic\n",
    "\n",
    "                replicated_parts = [df_microcell_part]\n",
    "\n",
    "                for provider_id in df_microcell['providerid'].unique():\n",
    "\n",
    "                    provider_remote = remote_data[remote_data['providerid'] == provider_id]\n",
    "                    candidate_microcells = []\n",
    "\n",
    "                    for _, row in microcell_coords.iterrows():\n",
    "                        if row['gen_microcell'] == assessing_mic:\n",
    "                            continue\n",
    "                        if (provider_remote['gen_microcell'] == row['gen_microcell']).any():\n",
    "                            dist = _haversine_km(lat1, lon1, row['latitude'], row['longitude'])\n",
    "                            candidate_microcells.append((row['gen_microcell'], dist))\n",
    "\n",
    "                    candidate_microcells.sort(key=lambda x: x[1])\n",
    "                    nearby_microcells = [m for m, _ in candidate_microcells[:38]]\n",
    "\n",
    "                    if nearby_microcells:\n",
    "                        df_remote = remote_data[\n",
    "                            (remote_data['providerid'] == provider_id) &\n",
    "                            (remote_data['gen_microcell'].isin(nearby_microcells))\n",
    "                        ].drop_duplicates(subset='serviceid')\n",
    "\n",
    "                        if not df_remote.empty:\n",
    "                            df_remote['origin'] = 'R'\n",
    "                            df_remote['currect_microcell'] = assessing_mic\n",
    "                            replicated_parts.append(df_remote)\n",
    "\n",
    "                df_microcell_replicated = pd.concat(replicated_parts, ignore_index=True)\n",
    "            else:\n",
    "                df_microcell_replicated = df_microcell.copy()\n",
    "\n",
    "            # -------- Local tampering (SPA) --------\n",
    "            spa_tampered_df = tampering.spa_tampering(\n",
    "                df_microcell_replicated,\n",
    "                type=tampering_type\n",
    "            )\n",
    "\n",
    "            # -------- Set TIMF data --------\n",
    "            data_service.set_local_data(spa_tampered_df.copy())\n",
    "\n",
    "            # --- Full remote pool (all other microcells) ---\n",
    "            remote_data_set = full_df[full_df['gen_microcell'] != assessing_mic].copy()\n",
    "\n",
    "            \n",
    "    \n",
    "            rng = np.random.default_rng(42)  # fixed seed for reproducibility\n",
    "            uni_mic = remote_data_set['gen_microcell'].unique()\n",
    "\n",
    "            selected_mics = rng.choice(\n",
    "                uni_mic,\n",
    "                size=max(1, int(len(uni_mic) * 1)),\n",
    "                replace=False\n",
    "            )\n",
    "\n",
    "            # --- Keep only records from selected microcells ---\n",
    "            remote_available = remote_data_set[remote_data_set['gen_microcell'].isin(selected_mics)].copy()\n",
    "\n",
    "            # --- Set remote data (partial availability) ---\n",
    "            data_service.set_remote_data(remote_available)\n",
    "\n",
    "            # print(spa_tampered_df.shape[0],bma_tampered_df.shape[0],df_microcell.shape[0]+bma_tampered_df.shape[0])\n",
    "\n",
    "            if assessing_mic == 'M110':\n",
    "                print(tampering_percentage,remote_data['true_label'].value_counts())\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f19c9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tampering_percentages = list(range(50, 60, 10))\n",
    "\n",
    "tampering_types = [\"N\", \"K\", \"S\"]  # Naive, Knowledgeable, Sophisticated\n",
    "# tampering_types = [\"N\"]\n",
    "        \n",
    "results = []\n",
    "\n",
    "results_metrics = []\n",
    "\n",
    "for tampering_type in tampering_types:\n",
    "    for tampering_percentage in tampering_percentages:\n",
    "\n",
    "        print(f\"Experiment: Tampering Type={tampering_type}, Percentage={tampering_percentage}%\")\n",
    "\n",
    "        for assessing_mic in data['gen_microcell'].unique():\n",
    "            full_df = data.copy()\n",
    "            df_microcell = data[data['gen_microcell'] == assessing_mic].copy()\n",
    "            remaining_df = data[data['gen_microcell'] != assessing_mic].copy()\n",
    "\n",
    "            # -------- Remote tampering (BMA) --------\n",
    "            bma_tampered_df = tampering.bma_tampering(\n",
    "                remaining_df.reset_index(drop=True),\n",
    "                tampering_percentage,\n",
    "                tampering_type\n",
    "            )\n",
    "            remote_data = bma_tampered_df.copy()\n",
    "\n",
    "            # remote_data = data[data['gen_microcell'] != assessing_mic].reset_index(drop=True)\n",
    "\n",
    "            # -------- Replication logic --------\n",
    "            microcell_coords = full_df.groupby('gen_microcell')[['latitude', 'longitude']].first().reset_index()\n",
    "            current_coords = microcell_coords[microcell_coords['gen_microcell'] == assessing_mic]\n",
    "\n",
    "            if not current_coords.empty:\n",
    "                lat1 = current_coords['latitude'].values[0]\n",
    "                lon1 = current_coords['longitude'].values[0]\n",
    "\n",
    "                df_microcell_part = df_microcell.copy()\n",
    "                df_microcell_part.loc[:, 'currect_microcell'] = assessing_mic\n",
    "\n",
    "                replicated_parts = [df_microcell_part]\n",
    "\n",
    "                for provider_id in df_microcell['providerid'].unique():\n",
    "\n",
    "                    provider_remote = remote_data[remote_data['providerid'] == provider_id]\n",
    "                    candidate_microcells = []\n",
    "\n",
    "                    for _, row in microcell_coords.iterrows():\n",
    "                        if row['gen_microcell'] == assessing_mic:\n",
    "                            continue\n",
    "                        if (provider_remote['gen_microcell'] == row['gen_microcell']).any():\n",
    "                            dist = _haversine_km(lat1, lon1, row['latitude'], row['longitude'])\n",
    "                            candidate_microcells.append((row['gen_microcell'], dist))\n",
    "\n",
    "                    candidate_microcells.sort(key=lambda x: x[1])\n",
    "                    nearby_microcells = [m for m, _ in candidate_microcells[:38]]\n",
    "\n",
    "                    if nearby_microcells:\n",
    "                        df_remote = remote_data[\n",
    "                            (remote_data['providerid'] == provider_id) &\n",
    "                            (remote_data['gen_microcell'].isin(nearby_microcells))\n",
    "                        ].drop_duplicates(subset='serviceid')\n",
    "\n",
    "                        if not df_remote.empty:\n",
    "                            df_remote['origin'] = 'R'\n",
    "                            df_remote['currect_microcell'] = assessing_mic\n",
    "                            replicated_parts.append(df_remote)\n",
    "\n",
    "                df_microcell_replicated = pd.concat(replicated_parts, ignore_index=True)\n",
    "            else:\n",
    "                df_microcell_replicated = df_microcell.copy()\n",
    "\n",
    "            # -------- Local tampering (SPA) --------\n",
    "            spa_tampered_df = tampering.spa_tampering(\n",
    "                df_microcell_replicated,\n",
    "                type=tampering_type\n",
    "            )\n",
    "\n",
    "            # -------- Set TIMF data --------\n",
    "            data_service.set_local_data(spa_tampered_df.copy())\n",
    "\n",
    "            # --- Full remote pool (all other microcells) ---\n",
    "            # remote_data_set = full_df[full_df['gen_microcell'] != assessing_mic].copy()\n",
    "\n",
    "            # remote_data_set = data[data['gen_microcell'] != assessing_mic].copy()\n",
    "\n",
    "            remote_data_set = bma_tampered_df.copy()\n",
    "\n",
    "\n",
    "\n",
    "            # --- Select 80% of microcells (availability by microcell) ---\n",
    "            rng = np.random.default_rng(42)  # fixed seed for reproducibility\n",
    "            uni_mic = remote_data_set['gen_microcell'].unique()\n",
    "\n",
    "            selected_mics = rng.choice(\n",
    "                uni_mic,\n",
    "                size=max(1, int(len(uni_mic) * 0.8)),\n",
    "                replace=False\n",
    "            )\n",
    "\n",
    "            # --- Keep only records from selected microcells ---\n",
    "            remote_available = remote_data_set[remote_data_set['gen_microcell'].isin(selected_mics)].copy()\n",
    "\n",
    "            # --- Set remote data (partial availability) ---\n",
    "            data_service.set_remote_data(remote_available)\n",
    "\n",
    "            # =====================================================\n",
    "            # Collect metrics per microcell across all providers\n",
    "            # =====================================================\n",
    "            tda_true, tda_pred = [], []\n",
    "            tda_time_entries = []\n",
    "\n",
    "            # ---- NEW: for AUC we need continuous scores\n",
    "            # Overall (optional)\n",
    "            tda_true_all, tda_score_all = [], []\n",
    "            # Part 1 (R)\n",
    "            tda_true_R, tda_score_R = [], []\n",
    "            # Part 2 (G)\n",
    "            tda_true_G, tda_score_G = [], []\n",
    "\n",
    "            baseline_true = {'IsolationForest': [], 'LOF': [], 'OCSVM': []}\n",
    "            baseline_pred = {'IsolationForest': [], 'LOF': [], 'OCSVM': []}\n",
    "            baseline_time_entries = {'IsolationForest': [], 'LOF': [], 'OCSVM': []}\n",
    "\n",
    "            # If your baselines also expose scores later, you can add baseline_score lists too.\n",
    "\n",
    "            for provider in df_microcell['providerid'].unique():\n",
    "\n",
    "                # ---------- TDA ----------\n",
    "                # IMPORTANT: df_tda must contain: origin, true_label, label, and score_R/score_G/score\n",
    "                _, df_tda, time_df = timf.trust_assessment(provider, assessing_mic)\n",
    "\n",
    "                if df_tda is not None and not df_tda.empty:\n",
    "                    # label-based metrics (existing)\n",
    "                    tda_true.extend(df_tda['true_label'].tolist())\n",
    "                    tda_pred.extend(df_tda['label'].tolist())\n",
    "\n",
    "                    # ---- AUC: collect continuous scores if present\n",
    "                    # Overall AUC (if you want it)\n",
    "                    if 'score' in df_tda.columns:\n",
    "                        m = df_tda['score'].notna()\n",
    "                        tda_true_all.extend(df_tda.loc[m, 'true_label'].tolist())\n",
    "                        tda_score_all.extend(df_tda.loc[m, 'score'].astype(float).tolist())\n",
    "\n",
    "                    # Part 1 AUC (received)\n",
    "                    if 'score_R' in df_tda.columns and 'origin' in df_tda.columns:\n",
    "                        m = (df_tda['origin'] == 'R') & (df_tda['score_R'].notna())\n",
    "                        tda_true_R.extend(df_tda.loc[m, 'true_label'].tolist())\n",
    "                        tda_score_R.extend(df_tda.loc[m, 'score_R'].astype(float).tolist())\n",
    "\n",
    "                    # Part 2 AUC (generated)\n",
    "                    if 'score_G' in df_tda.columns and 'origin' in df_tda.columns:\n",
    "                        m = (df_tda['origin'] == 'G') & (df_tda['score_G'].notna())\n",
    "                        tda_true_G.extend(df_tda.loc[m, 'true_label'].tolist())\n",
    "                        tda_score_G.extend(df_tda.loc[m, 'score_G'].astype(float).tolist())\n",
    "\n",
    "                if isinstance(time_df, dict):\n",
    "                    tda_time_entries.append({\n",
    "                        'records': time_df.get('records', np.nan),\n",
    "                        'time_sec': time_df.get('time', np.nan)\n",
    "                    })\n",
    "\n",
    "                # ---------- Baseline: IsolationForest ----------\n",
    "                df_if, t_if = timf.trust_assessment_baseline_ifum(provider, assessing_mic)\n",
    "\n",
    "                if df_if is not None and not df_if.empty:\n",
    "                    baseline_true['IsolationForest'].extend(df_if['true_label'].tolist())\n",
    "                    baseline_pred['IsolationForest'].extend(df_if['label'].tolist())\n",
    "\n",
    "                if isinstance(t_if, dict):\n",
    "                    baseline_time_entries['IsolationForest'].append({\n",
    "                        'records': t_if.get('records', np.nan),\n",
    "                        'time_sec': t_if.get('time', np.nan)\n",
    "                    })\n",
    "\n",
    "                # ---------- Baseline: LOF ----------\n",
    "                df_lof, t_lof = timf.trust_assessment_baseline_lof(provider, assessing_mic)\n",
    "\n",
    "                if df_lof is not None and not df_lof.empty:\n",
    "                    baseline_true['LOF'].extend(df_lof['true_label'].tolist())\n",
    "                    baseline_pred['LOF'].extend(df_lof['label'].tolist())\n",
    "\n",
    "                if isinstance(t_lof, dict):\n",
    "                    baseline_time_entries['LOF'].append({\n",
    "                        'records': t_lof.get('records', np.nan),\n",
    "                        'time_sec': t_lof.get('time', np.nan)\n",
    "                    })\n",
    "\n",
    "                # ---------- Baseline: OCSVM ----------\n",
    "                df_oc, t_oc = timf.trust_assessment_baseline_ocsvm(provider, assessing_mic)\n",
    "\n",
    "                if df_oc is not None and not df_oc.empty:\n",
    "                    baseline_true['OCSVM'].extend(df_oc['true_label'].tolist())\n",
    "                    baseline_pred['OCSVM'].extend(df_oc['label'].tolist())\n",
    "\n",
    "                if isinstance(t_oc, dict):\n",
    "                    baseline_time_entries['OCSVM'].append({\n",
    "                        'records': t_oc.get('records', np.nan),\n",
    "                        'time_sec': t_oc.get('time', np.nan)\n",
    "                    })\n",
    "\n",
    "            # =====================================================\n",
    "            # Save rows (TDA + baselines)\n",
    "            # =====================================================\n",
    "            rows_to_add = []\n",
    "\n",
    "            # ---- TDA row\n",
    "            if len(tda_true) > 0:\n",
    "                acc, prec, rec = compute_metrics_from_labels(tda_true, tda_pred)\n",
    "\n",
    "                # ---- NEW: AUCs from scores\n",
    "                auc_all = safe_auc(tda_true_all, tda_score_all)  # optional\n",
    "                auc_R = safe_auc(tda_true_R, tda_score_R)\n",
    "                auc_G = safe_auc(tda_true_G, tda_score_G)\n",
    "\n",
    "                rows_to_add.append({\n",
    "                    'tampering_type': tampering_type,\n",
    "                    'tampering_percentage': tampering_percentage,\n",
    "                    'microcell': assessing_mic,\n",
    "                    'provider_id': 'ALL',\n",
    "                    'method': 'TDA',\n",
    "                    'accuracy': acc,\n",
    "                    'precision': prec,\n",
    "                    'recall': rec,\n",
    "                    'AUC_all': auc_all,\n",
    "                    'AUC_R': auc_R,\n",
    "                    'AUC_G': auc_G,\n",
    "                    'time_records': tda_time_entries\n",
    "                })\n",
    "\n",
    "            # ---- Baseline rows (label-based AUC not possible unless you return scores)\n",
    "            # If your baseline wrappers later return a continuous score column (recommended),\n",
    "            # compute AUC the same way as above using safe_auc(y_true, y_score).\n",
    "            for method_name in ['IsolationForest', 'LOF', 'OCSVM']:\n",
    "                if len(baseline_true[method_name]) == 0:\n",
    "                    continue\n",
    "\n",
    "                acc, prec, rec = compute_metrics_from_labels(\n",
    "                    baseline_true[method_name],\n",
    "                    baseline_pred[method_name]\n",
    "                )\n",
    "\n",
    "                rows_to_add.append({\n",
    "                    'tampering_type': tampering_type,\n",
    "                    'tampering_percentage': tampering_percentage,\n",
    "                    'microcell': assessing_mic,\n",
    "                    'provider_id': 'ALL',\n",
    "                    'method': method_name,\n",
    "                    'accuracy': acc,\n",
    "                    'precision': prec,\n",
    "                    'recall': rec,\n",
    "                    # AUC requires continuous scores; keep as NaN unless you return scores\n",
    "                    'AUC_all': np.nan,\n",
    "                    'AUC_R': np.nan,\n",
    "                    'AUC_G': np.nan,\n",
    "                    'time_records': baseline_time_entries[method_name]\n",
    "                })\n",
    "\n",
    "            results_metrics.extend(rows_to_add)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91307c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c73afd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "\n",
    "# Pretty names for methods (legend)\n",
    "method_labels = {\n",
    "    \n",
    "    \"TDA\": \"TDA (our approch)\",\n",
    "    \"IsolationForest\": \"IFUM\",\n",
    "    \"LOF\": \"LOFM\",\n",
    "    \"OCSVM\": \"OCSVM\",\n",
    "    \"TIMF\": \"TIMF\"\n",
    "}\n",
    "\n",
    "# Pretty names for tampering types (title)\n",
    "tampering_labels = {\n",
    "    \"N\": \"Naïve Attack\",\n",
    "    \"K\": \"Knowledge-based Attack\",\n",
    "    \"S\": \"Sophisticated Attack\"\n",
    "}\n",
    "\n",
    "# Pretty names for metrics (subplot titles / y-axis)\n",
    "metric_labels = {\n",
    "    \"accuracy\": \"(A) Accuracy\",\n",
    "    \"precision\": \"(B) Precision\",\n",
    "    \"recall\": \"(R) Recall\"\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Prepare the DataFrame\n",
    "metrics_df = pd.DataFrame(results_metrics).dropna(\n",
    "    subset=['accuracy', 'precision', 'recall']\n",
    ")\n",
    "\n",
    "# Compute mean metrics\n",
    "summary = (\n",
    "    metrics_df\n",
    "    .groupby(['tampering_type', 'tampering_percentage', 'method'], as_index=False)\n",
    "    [['accuracy', 'precision', 'recall']]\n",
    "    .mean()\n",
    ")\n",
    "\n",
    "tampering_types = [\"N\", \"K\", \"S\"]\n",
    "metric_names = ['accuracy', 'precision', 'recall']\n",
    "\n",
    "# Marker and line-style cycles (color-blind friendly)\n",
    "markers = cycle(['o', 's', '^', 'D',])\n",
    "linestyles = cycle(['-', '--', '-.', ':'])\n",
    "\n",
    "method_colors = {\n",
    "    \"TDA\": \"tab:green\",          # proposed method\n",
    "    \"IsolationForest\": \"tab:orange\",\n",
    "    \"LOF\": \"tab:blue\",\n",
    "    \"OCSVM\": \"tab:red\",\n",
    "    \"TIMF\": \"tab:purple\"\n",
    "}\n",
    "\n",
    "\n",
    "# Loop over each tampering type\n",
    "for t_type in tampering_types:\n",
    "    subset_type = summary[summary['tampering_type'] == t_type]\n",
    "    methods = subset_type['method'].unique()\n",
    "\n",
    "    # Assign a unique (marker, linestyle) per method\n",
    "    style_map = {\n",
    "        method: (next(markers), next(linestyles))\n",
    "        for method in methods\n",
    "    }\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5), sharex=True)\n",
    "    # fig.suptitle(f'Metrics for Tampering Type {t_type}', fontsize=16)\n",
    "\n",
    "    for idx, metric in enumerate(metric_names):\n",
    "        ax = axes[idx]\n",
    "\n",
    "        for method in methods:\n",
    "            subset_method = subset_type[subset_type['method'] == method]\n",
    "            marker, linestyle = style_map[method]\n",
    "\n",
    "            ax.plot(\n",
    "                subset_method['tampering_percentage'],\n",
    "                subset_method[metric],\n",
    "                marker=marker,\n",
    "                linestyle=linestyle,\n",
    "                linewidth=2,\n",
    "                markersize=6,\n",
    "                label=method_labels.get(method, method),\n",
    "                 color=method_colors.get(method, None)  # <-- force blue for TDA\n",
    "            )\n",
    "\n",
    "        ax.set_title(metric.capitalize())\n",
    "        ax.set_xlabel('Tampering Percentage')\n",
    "        ax.set_ylabel(metric.capitalize())\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "    axes[0].legend(title='Method', frameon=True)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5106608a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "\n",
    "# Prepare the DataFrame\n",
    "metrics_df = pd.DataFrame(results_metrics).dropna(\n",
    "    subset=['accuracy', 'precision', 'recall']\n",
    ")\n",
    "\n",
    "# Compute mean metrics\n",
    "summary = (\n",
    "    metrics_df\n",
    "    .groupby(['tampering_type', 'tampering_percentage', 'method'], as_index=False)\n",
    "    [['accuracy', 'precision', 'recall']]\n",
    "    .mean()\n",
    ")\n",
    "\n",
    "tampering_types = [\"N\", \"K\", \"S\"]\n",
    "metric_names = ['accuracy', 'precision', 'recall']\n",
    "\n",
    "# Marker and line-style cycles (color-blind friendly)\n",
    "markers = cycle(['o', 's', '^', 'D', 'X', 'P', 'v'])\n",
    "linestyles = cycle(['-', '--', '-.', ':'])\n",
    "\n",
    "# Loop over each tampering type\n",
    "for t_type in tampering_types:\n",
    "    subset_type = summary[summary['tampering_type'] == t_type]\n",
    "    methods = subset_type['method'].unique()\n",
    "\n",
    "    # Assign a unique (marker, linestyle) per method\n",
    "    style_map = {\n",
    "        method: (next(markers), next(linestyles))\n",
    "        for method in methods\n",
    "    }\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5), sharex=True)\n",
    "    # fig.suptitle(f'Metrics for Tampering Type {t_type}', fontsize=16)\n",
    "\n",
    "    for idx, metric in enumerate(metric_names):\n",
    "        ax = axes[idx]\n",
    "\n",
    "        for method in methods:\n",
    "            subset_method = subset_type[subset_type['method'] == method]\n",
    "            marker, linestyle = style_map[method]\n",
    "\n",
    "            ax.plot(\n",
    "                subset_method['tampering_percentage'],\n",
    "                subset_method[metric],\n",
    "                marker=marker,\n",
    "                linestyle=linestyle,\n",
    "                linewidth=2,\n",
    "                markersize=6,\n",
    "                label=method\n",
    "            )\n",
    "\n",
    "        ax.set_title(metric.capitalize())\n",
    "        ax.set_xlabel('Tampering Percentage')\n",
    "        ax.set_ylabel(metric.capitalize())\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "    axes[0].legend(title='Method', frameon=True)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99849288",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7bf47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d0bfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "df_box = df_tda[['tampering_percentage', 'AUC_R', 'AUC_G']].melt(\n",
    "    id_vars='tampering_percentage',\n",
    "    value_vars=['AUC_R', 'AUC_G'],\n",
    "    var_name='Stage',\n",
    "    value_name='AUC'\n",
    ")\n",
    "\n",
    "df_box.dropna(inplace=True)\n",
    "\n",
    "for stage, marker in zip(['AUC_R', 'AUC_G'], ['o', 's']):\n",
    "    subset = df_box[df_box['Stage'] == stage]\n",
    "    plt.scatter(subset['tampering_percentage'], subset['AUC'], alpha=0.4, label=stage)\n",
    "\n",
    "plt.xlabel('Tampering Percentage (%)')\n",
    "plt.ylabel('AUC')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add8cfc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paper1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
