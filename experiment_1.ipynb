{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94c0615f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded existing evaluation data from CSV.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from evaluations.evaluation_data.evaluation_data import EvaluationData\n",
    "from tampering.tampering import Tampering\n",
    "from timf.timf import TIMF\n",
    "from data_service.data_service import DataService\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "evaluation_data = EvaluationData()\n",
    "tampering = Tampering()\n",
    "        \n",
    "data_service = DataService()\n",
    "timf = TIMF(data_service)\n",
    "\n",
    "data = evaluation_data.get_data()\n",
    "\n",
    "\n",
    "tampering_percentages = list(range(10, 100, 20))\n",
    "\n",
    "tampering_types = [\"N\", \"K\", \"S\"]  # Naive, Knowledgeable, Sophisticated\n",
    "# tampering_types = [\"N\"]\n",
    "        \n",
    "results = []\n",
    "\n",
    "def _haversine_km(lat1, lon1, lat2, lon2):\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    return 6371.0 * c\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\n",
    "\n",
    "def compute_metrics_from_labels(y_true_labels, y_pred_labels):\n",
    "    \"\"\"\n",
    "    Inputs are arrays/Series with values 'T' or 'C'\n",
    "    \"\"\"\n",
    "    y_true = (pd.Series(y_true_labels) == 'T').astype(int)\n",
    "    y_pred = (pd.Series(y_pred_labels) == 'T').astype(int)\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "    try:\n",
    "        auc = roc_auc_score(y_true, y_pred)\n",
    "    except ValueError:\n",
    "        auc = None\n",
    "\n",
    "    return accuracy, precision, recall, auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f19c9da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment: Tampering Type=N, Percentage=10%\n",
      "Experiment: Tampering Type=N, Percentage=30%\n",
      "Experiment: Tampering Type=N, Percentage=50%\n",
      "Experiment: Tampering Type=N, Percentage=70%\n",
      "Experiment: Tampering Type=N, Percentage=90%\n",
      "Experiment: Tampering Type=K, Percentage=10%\n",
      "Experiment: Tampering Type=K, Percentage=30%\n",
      "Experiment: Tampering Type=K, Percentage=50%\n"
     ]
    }
   ],
   "source": [
    "results_metrics = []\n",
    "\n",
    "for tampering_type in tampering_types:\n",
    "    for tampering_percentage in tampering_percentages:\n",
    "\n",
    "        print(f\"Experiment: Tampering Type={tampering_type}, Percentage={tampering_percentage}%\")\n",
    "\n",
    "        for assessing_mic in data['gen_microcell'].unique():\n",
    "\n",
    "            df_microcell = data[data['gen_microcell'] == assessing_mic]\n",
    "\n",
    "            # -------- Remote tampering (BMA) --------\n",
    "            bma_tampered_df = tampering.bma_tampering(\n",
    "                data[data['gen_microcell'] != assessing_mic].reset_index(drop=True),\n",
    "                tampering_percentage,\n",
    "                tampering_type\n",
    "            )\n",
    "\n",
    "            remote_data = bma_tampered_df.copy()\n",
    "\n",
    "            # -------- Replication logic --------\n",
    "            microcell_coords = data.groupby('gen_microcell')[['latitude', 'longitude']].first().reset_index()\n",
    "            current_coords = microcell_coords[microcell_coords['gen_microcell'] == assessing_mic]\n",
    "\n",
    "            if not current_coords.empty:\n",
    "                lat1 = current_coords['latitude'].values[0]\n",
    "                lon1 = current_coords['longitude'].values[0]\n",
    "\n",
    "                df_microcell_part = df_microcell.copy()\n",
    "                df_microcell_part.loc[:, 'currect_microcell'] = assessing_mic\n",
    "\n",
    "                replicated_parts = [df_microcell_part]\n",
    "\n",
    "                for provider_id in df_microcell['providerid'].unique():\n",
    "\n",
    "                    provider_remote = remote_data[remote_data['providerid'] == provider_id]\n",
    "                    candidate_microcells = []\n",
    "\n",
    "                    for _, row in microcell_coords.iterrows():\n",
    "                        if row['gen_microcell'] == assessing_mic:\n",
    "                            continue\n",
    "                        if (provider_remote['gen_microcell'] == row['gen_microcell']).any():\n",
    "                            dist = _haversine_km(lat1, lon1, row['latitude'], row['longitude'])\n",
    "                            candidate_microcells.append((row['gen_microcell'], dist))\n",
    "\n",
    "                    candidate_microcells.sort(key=lambda x: x[1])\n",
    "                    nearby_microcells = [m for m, _ in candidate_microcells[:10]]\n",
    "\n",
    "                    if nearby_microcells:\n",
    "                        df_remote = remote_data[\n",
    "                            (remote_data['providerid'] == provider_id) &\n",
    "                            (remote_data['gen_microcell'].isin(nearby_microcells))\n",
    "                        ].drop_duplicates(subset='serviceid')\n",
    "\n",
    "                        if not df_remote.empty:\n",
    "                            df_remote['origin'] = 'R'\n",
    "                            df_remote['currect_microcell'] = assessing_mic\n",
    "                            replicated_parts.append(df_remote)\n",
    "\n",
    "                df_microcell_replicated = pd.concat(replicated_parts, ignore_index=True)\n",
    "            else:\n",
    "                df_microcell_replicated = df_microcell.copy()\n",
    "\n",
    "            # -------- Local tampering (SPA) --------\n",
    "            spa_tampered_df = tampering.spa_tampering(\n",
    "                df_microcell_replicated,\n",
    "                type=tampering_type\n",
    "            )\n",
    "\n",
    "            # -------- Set TIMF data --------\n",
    "            data_service.set_local_data(spa_tampered_df.copy())\n",
    "            data_service.set_remote_data(bma_tampered_df.copy())\n",
    "            # data_service.set_remote_data(data[data['gen_microcell'] != assessing_mic].copy())\n",
    "\n",
    "            # =====================================================\n",
    "            # TDA: COLLECT labels across providers\n",
    "            # =====================================================\n",
    "            tda_true, tda_pred = [], []\n",
    "\n",
    "            for provider in df_microcell['providerid'].unique():\n",
    "                _, df_tda,time_df = timf.trust_assessment(provider, assessing_mic)\n",
    "\n",
    "                if not df_tda.empty:\n",
    "                    tda_true.extend(df_tda['true_label'])\n",
    "                    tda_pred.extend(df_tda['label'])\n",
    "\n",
    "            if len(tda_true) > 0:\n",
    "                acc, prec, rec, auc = compute_metrics_from_labels(tda_true, tda_pred)\n",
    "\n",
    "                results_metrics.append({\n",
    "                    'tampering_type': tampering_type,\n",
    "                    'tampering_percentage': tampering_percentage,\n",
    "                    'microcell': assessing_mic,\n",
    "                    'provider_id': 'ALL',\n",
    "                    'method': 'TDA',\n",
    "                    'accuracy': acc,\n",
    "                    'precision': prec,\n",
    "                    'recall': rec,\n",
    "                    'AUC': auc,\n",
    "                    'timetaken':time_df\n",
    "\n",
    "                })\n",
    "\n",
    "            # =====================================================\n",
    "            # BASELINES: same aggregation logic\n",
    "            # =====================================================\n",
    "            baseline_true = {}\n",
    "            baseline_pred = {}\n",
    "\n",
    "            for provider in df_microcell['providerid'].unique():\n",
    "                baseline_results = timf.trust_assessment_baseline(provider, assessing_mic)\n",
    "\n",
    "                for method_name, df_base in baseline_results.items():\n",
    "                    if df_base.empty:\n",
    "                        continue\n",
    "\n",
    "                    baseline_true.setdefault(method_name, []).extend(df_base['true_label'])\n",
    "                    baseline_pred.setdefault(method_name, []).extend(df_base['label'])\n",
    "\n",
    "            for method_name in baseline_true.keys():\n",
    "                acc, prec, rec, auc = compute_metrics_from_labels(\n",
    "                    baseline_true[method_name],\n",
    "                    baseline_pred[method_name]\n",
    "                )\n",
    "\n",
    "                results_metrics.append({\n",
    "                    'tampering_type': tampering_type,\n",
    "                    'tampering_percentage': tampering_percentage,\n",
    "                    'microcell': assessing_mic,\n",
    "                    'provider_id': 'ALL',\n",
    "                    'method': method_name,\n",
    "                    'accuracy': acc,\n",
    "                    'precision': prec,\n",
    "                    'recall': rec,\n",
    "                    'AUC': auc\n",
    "                })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91307c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c73afd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "\n",
    "# Prepare the DataFrame\n",
    "metrics_df = pd.DataFrame(results_metrics).dropna(\n",
    "    subset=['accuracy', 'precision', 'recall']\n",
    ")\n",
    "\n",
    "# Compute mean metrics\n",
    "summary = (\n",
    "    metrics_df\n",
    "    .groupby(['tampering_type', 'tampering_percentage', 'method'], as_index=False)\n",
    "    [['accuracy', 'precision', 'recall']]\n",
    "    .mean()\n",
    ")\n",
    "\n",
    "tampering_types = [\"N\", \"K\", \"S\"]\n",
    "metric_names = ['accuracy', 'precision', 'recall']\n",
    "\n",
    "# Marker and line-style cycles (color-blind friendly)\n",
    "markers = cycle(['o', 's', '^', 'D', 'X', 'P', 'v'])\n",
    "linestyles = cycle(['-', '--', '-.', ':'])\n",
    "\n",
    "# Loop over each tampering type\n",
    "for t_type in tampering_types:\n",
    "    subset_type = summary[summary['tampering_type'] == t_type]\n",
    "    methods = subset_type['method'].unique()\n",
    "\n",
    "    # Assign a unique (marker, linestyle) per method\n",
    "    style_map = {\n",
    "        method: (next(markers), next(linestyles))\n",
    "        for method in methods\n",
    "    }\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5), sharex=True)\n",
    "    fig.suptitle(f'Metrics for Tampering Type {t_type}', fontsize=16)\n",
    "\n",
    "    for idx, metric in enumerate(metric_names):\n",
    "        ax = axes[idx]\n",
    "\n",
    "        for method in methods:\n",
    "            subset_method = subset_type[subset_type['method'] == method]\n",
    "            marker, linestyle = style_map[method]\n",
    "\n",
    "            ax.plot(\n",
    "                subset_method['tampering_percentage'],\n",
    "                subset_method[metric],\n",
    "                marker=marker,\n",
    "                linestyle=linestyle,\n",
    "                linewidth=2,\n",
    "                markersize=6,\n",
    "                label=method\n",
    "            )\n",
    "\n",
    "        ax.set_title(metric.capitalize())\n",
    "        ax.set_xlabel('Tampering Percentage')\n",
    "        ax.set_ylabel(metric.capitalize())\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "    axes[0].legend(title='Method', frameon=True)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b487a713",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df_time = pd.DataFrame(results_metrics)\n",
    "print(df_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2ba36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_records(x):\n",
    "    # if timetaken is dict -> pull records\n",
    "    if isinstance(x, dict):\n",
    "        return x.get('records', np.nan)\n",
    "    # if it's already a number -> records unknown\n",
    "    return np.nan\n",
    "\n",
    "def get_time(x):\n",
    "    # if timetaken is dict -> pull time\n",
    "    if isinstance(x, dict):\n",
    "        return x.get('time', np.nan)\n",
    "    # if it's already a number -> that's the time\n",
    "    if isinstance(x, (int, float, np.number)):\n",
    "        return float(x)\n",
    "    return np.nan\n",
    "\n",
    "df_time['records'] = df_time['timetaken'].apply(get_records)\n",
    "df_time['time_sec'] = df_time['timetaken'].apply(get_time)\n",
    "\n",
    "# keep only rows where both records + time exist\n",
    "df_plot = df_time.dropna(subset=['records', 'time_sec']).copy()\n",
    "df_plot['records'] = df_plot['records'].astype(int)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(df_plot['records'], df_plot['time_sec'], marker='o')\n",
    "plt.xlabel('Number of Records')\n",
    "plt.ylabel('Time Taken (seconds)')\n",
    "plt.title('Records vs Time')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3380d5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bb8f0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afe215f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d96f7ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0fd023",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2733fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f954ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c7e1ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce116b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\n",
    "\n",
    "\n",
    "results_metrics = []\n",
    "\n",
    "\n",
    "for tampering_type in tampering_types:\n",
    "    for tampering_percentage in tampering_percentages:\n",
    "        print(f\"Experiment: Tampering Type={tampering_type}, Percentage={tampering_percentage}%\")\n",
    "        for assessing_mic in data['gen_microcell'].unique():\n",
    "            df_microcell = data[data['gen_microcell'] == assessing_mic]\n",
    "            # print(df_microcell)\n",
    "\n",
    "            bma_tampered_df = tampering.bma_tampering(\n",
    "                        data[data['gen_microcell'] != assessing_mic].reset_index(drop=True),\n",
    "                        tampering_percentage,\n",
    "                        tampering_type\n",
    "                    )\n",
    "            \n",
    "            \n",
    "\n",
    "            remote_data = bma_tampered_df.copy()\n",
    "\n",
    "            # Replicate data: for each provider in df_microcell, pull records from 10 nearest microcells\n",
    "            microcell_coords = data.groupby('gen_microcell')[['latitude', 'longitude']].first().reset_index()\n",
    "            current_coords = microcell_coords[microcell_coords['gen_microcell'] == assessing_mic]\n",
    "            if not current_coords.empty:\n",
    "                lat1 = current_coords['latitude'].values[0]\n",
    "                lon1 = current_coords['longitude'].values[0]\n",
    "\n",
    "                df_microcell_part = df_microcell.copy()\n",
    "                df_microcell_part.loc[:, 'currect_microcell'] = assessing_mic\n",
    "                \n",
    "                # df_microcell['currect_microcell'] = assessing_mic\n",
    "                replicated_parts = [df_microcell_part]\n",
    "                for provider_id in df_microcell['providerid'].unique():\n",
    "                    candidate_microcells = []\n",
    "                    provider_remote = remote_data[remote_data['providerid'] == provider_id]\n",
    "                    for _, row in microcell_coords.iterrows():\n",
    "                        if row['gen_microcell'] == assessing_mic:\n",
    "                            continue\n",
    "                        if (provider_remote['gen_microcell'] == row['gen_microcell']).any():\n",
    "                            dist = _haversine_km(lat1, lon1, row['latitude'], row['longitude'])\n",
    "                            candidate_microcells.append((row['gen_microcell'], dist))\n",
    "\n",
    "                    if candidate_microcells:\n",
    "                        candidate_microcells.sort(key=lambda x: x[1])\n",
    "                        nearby_microcells = [m for m, _ in candidate_microcells[:10]]\n",
    "                    else:\n",
    "                        nearby_microcells = []\n",
    "\n",
    "                    if nearby_microcells:\n",
    "                        remote_mask = (\n",
    "                            (remote_data['providerid'] == provider_id) &\n",
    "                            (remote_data['gen_microcell'].isin(nearby_microcells)) &\n",
    "                            (remote_data['gen_microcell'] != assessing_mic)\n",
    "                        )\n",
    "                        df_remote = remote_data.loc[remote_mask].copy()\n",
    "                        if not df_remote.empty:\n",
    "                            df_remote = df_remote.drop_duplicates(subset='serviceid')\n",
    "                            df_remote['origin'] = 'R'\n",
    "                            df_remote['currect_microcell'] = assessing_mic\n",
    "                            replicated_parts.append(df_remote)\n",
    "\n",
    "                df_microcell_replicated = pd.concat(replicated_parts, ignore_index=True)\n",
    "            else:\n",
    "                df_microcell_replicated = df_microcell.copy()\n",
    "\n",
    "            #df_microcell_replicated contain local generated data, and data gathered from 10 remte microcells per provider now lets assume the lacal super provider is also tamper with the data. \n",
    "\n",
    "            spa_tampered_df = tampering.spa_tampering(\n",
    "                        df_microcell_replicated,\n",
    "                        type=tampering_type\n",
    "                    )\n",
    "            \n",
    "            # if assessing_mic=='M102':\n",
    "            #     print(spa_tampered_df['currect_microcell'].unique())\n",
    "\n",
    "            #     spa_tampered_df.to_csv(\"test.csv\",index=False)\n",
    "            #     bma_tampered_df.to_csv(\"test2.csv\",index=False)\n",
    "\n",
    "                \n",
    "\n",
    "            #     print(df_microcell_replicated['gen_microcell'].unique())\n",
    "            # else:\n",
    "            #     continue\n",
    "\n",
    "\n",
    "            \n",
    "            data_service.set_local_data(spa_tampered_df.copy())\n",
    "            data_service.set_remote_data(bma_tampered_df.copy())\n",
    "            # data_service.set_remote_data(data[data['gen_microcell'] != assessing_mic].copy())\n",
    "            \n",
    "\n",
    "            # data service is set with the tamperedcx data which will be received by the timf\n",
    "\n",
    "            # for each provider lets evaluate the trust score\n",
    "            for provider in df_microcell['providerid'].unique():\n",
    "                       \n",
    "                trust_score, df = timf.trust_assessment(provider, assessing_mic)\n",
    "                if not df.empty and 'true_label' in df.columns and 'label' in df.columns:\n",
    "                    y_true = df['true_label']\n",
    "                    y_pred = df['label']\n",
    "                        \n",
    "                        # Binary: T = 1 (tampered), C = 0 (correct)\n",
    "                    y_true_bin = (y_true == 'T').astype(int)\n",
    "                    y_pred_bin = (y_pred == 'T').astype(int)\n",
    "                        \n",
    "                    tp = ((y_true_bin == 1) & (y_pred_bin == 1)).sum()\n",
    "                    fp = ((y_true_bin == 0) & (y_pred_bin == 1)).sum()\n",
    "                    tn = ((y_true_bin == 0) & (y_pred_bin == 0)).sum()\n",
    "                    fn = ((y_true_bin == 1) & (y_pred_bin == 0)).sum()\n",
    "                        \n",
    "                    total = len(y_true_bin)\n",
    "                    accuracy = (tp + tn) / total if total > 0 else 0.0\n",
    "                    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "                    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "\n",
    "                    try:\n",
    "                        auc = roc_auc_score(y_true_bin, y_pred_bin)\n",
    "                    except ValueError:\n",
    "                        auc = None \n",
    "                        \n",
    "\n",
    "                    results_metrics.append({\n",
    "                            'tampering_type': tampering_type,\n",
    "                            'tampering_percentage': tampering_percentage,\n",
    "                            'microcell': assessing_mic,\n",
    "                            'provider_id': provider,\n",
    "                            'method': 'TDA',\n",
    "                            'accuracy': accuracy,\n",
    "                            'precision': precision,\n",
    "                            'recall': recall,\n",
    "                            'AUC': auc\n",
    "                        })\n",
    "                else:\n",
    "                    print(\"issue with df for metrics calculation\")\n",
    "\n",
    "                \n",
    "                baseline_results = timf.trust_assessment_baseline(provider, assessing_mic)\n",
    "    \n",
    "\n",
    "                for method_name, df_baseline in baseline_results.items():\n",
    "                    if not df_baseline.empty and 'true_label' in df_baseline.columns and 'label' in df_baseline.columns:\n",
    "\n",
    "                        # y_true = df['true_label']\n",
    "                        # y_pred = df['label']\n",
    "                            \n",
    "                        #     # Binary: T = 1 (tampered), C = 0 (correct)\n",
    "                        # y_true_bin = (y_true == 'T').astype(int)\n",
    "                        # y_pred_bin = (y_pred == 'T').astype(int)\n",
    "                            \n",
    "                        # tp = ((y_true_bin == 1) & (y_pred_bin == 1)).sum()\n",
    "                        # fp = ((y_true_bin == 0) & (y_pred_bin == 1)).sum()\n",
    "                        # tn = ((y_true_bin == 0) & (y_pred_bin == 0)).sum()\n",
    "                        # fn = ((y_true_bin == 1) & (y_pred_bin == 0)).sum()\n",
    "                            \n",
    "                        # total = len(y_true_bin)\n",
    "                        # accuracy = (tp + tn) / total if total > 0 else 0.0\n",
    "                        # precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "                        # recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "\n",
    "\n",
    "\n",
    "                        y_true = (df_baseline['true_label'] == 'T').astype(int)  # 1 = tampered, 0 = correct\n",
    "                        y_pred = (df_baseline['label'] == 'T').astype(int)\n",
    "\n",
    "                        accuracy = accuracy_score(y_true, y_pred)\n",
    "                        precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "                        recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "                        try:\n",
    "                            auc2 = roc_auc_score(y_true_bin, y_pred_bin)\n",
    "                        except ValueError:\n",
    "                            auc2 = None\n",
    "\n",
    "\n",
    "                        results_metrics.append({\n",
    "                            'tampering_type': tampering_type,\n",
    "                            'tampering_percentage': tampering_percentage,\n",
    "                            'microcell': assessing_mic,\n",
    "                            'provider_id': provider,\n",
    "                            'method': method_name,  # specify which baseline\n",
    "                            'accuracy': accuracy,\n",
    "                            'precision': precision,\n",
    "                            'recall': recall,\n",
    "                            'AUC':auc2\n",
    "\n",
    "                        })\n",
    "                    else:\n",
    "                        print(\"issue with df for metrics calculation\")\n",
    "\n",
    "\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ce2137",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccc78a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "\n",
    "# Prepare the DataFrame\n",
    "metrics_df = pd.DataFrame(results_metrics).dropna(\n",
    "    subset=['accuracy', 'precision', 'recall']\n",
    ")\n",
    "\n",
    "# Compute mean metrics\n",
    "summary = (\n",
    "    metrics_df\n",
    "    .groupby(['tampering_type', 'tampering_percentage', 'method'], as_index=False)\n",
    "    [['accuracy', 'precision', 'recall']]\n",
    "    .mean()\n",
    ")\n",
    "\n",
    "tampering_types = [\"N\", \"K\", \"S\"]\n",
    "metric_names = ['accuracy', 'precision', 'recall']\n",
    "\n",
    "# Marker and line-style cycles (color-blind friendly)\n",
    "markers = cycle(['o', 's', '^', 'D', 'X', 'P', 'v'])\n",
    "linestyles = cycle(['-', '--', '-.', ':'])\n",
    "\n",
    "# Loop over each tampering type\n",
    "for t_type in tampering_types:\n",
    "    subset_type = summary[summary['tampering_type'] == t_type]\n",
    "    methods = subset_type['method'].unique()\n",
    "\n",
    "    # Assign a unique (marker, linestyle) per method\n",
    "    style_map = {\n",
    "        method: (next(markers), next(linestyles))\n",
    "        for method in methods\n",
    "    }\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5), sharex=True)\n",
    "    fig.suptitle(f'Metrics for Tampering Type {t_type}', fontsize=16)\n",
    "\n",
    "    for idx, metric in enumerate(metric_names):\n",
    "        ax = axes[idx]\n",
    "\n",
    "        for method in methods:\n",
    "            subset_method = subset_type[subset_type['method'] == method]\n",
    "            marker, linestyle = style_map[method]\n",
    "\n",
    "            ax.plot(\n",
    "                subset_method['tampering_percentage'],\n",
    "                subset_method[metric],\n",
    "                marker=marker,\n",
    "                linestyle=linestyle,\n",
    "                linewidth=2,\n",
    "                markersize=6,\n",
    "                label=method\n",
    "            )\n",
    "\n",
    "        ax.set_title(metric.capitalize())\n",
    "        ax.set_xlabel('Tampering Percentage')\n",
    "        ax.set_ylabel(metric.capitalize())\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "    axes[0].legend(title='Method', frameon=True)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a029dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6305bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\n",
    "\n",
    "results_metrics = []\n",
    "\n",
    "for tampering_type in tampering_types:\n",
    "    for tampering_percentage in tampering_percentages:\n",
    "        print(f\"Experiment: Tampering Type={tampering_type}, Percentage={tampering_percentage}%\")\n",
    "        for assessing_mic in data['gen_microcell'].unique():\n",
    "\n",
    "            df_microcell = data[data['gen_microcell'] == assessing_mic]\n",
    "            # print(df_microcell)\n",
    "\n",
    "            bma_tampered_df = tampering.bma_tampering(\n",
    "                        data[data['gen_microcell'] != assessing_mic].reset_index(drop=True),\n",
    "                        tampering_percentage,\n",
    "                        tampering_type\n",
    "                    )\n",
    "            \n",
    "            \n",
    "\n",
    "            remote_data = bma_tampered_df.copy()\n",
    "\n",
    "            # Replicate data: for each provider in df_microcell, pull records from 10 nearest microcells\n",
    "            microcell_coords = data.groupby('gen_microcell')[['latitude', 'longitude']].first().reset_index()\n",
    "            current_coords = microcell_coords[microcell_coords['gen_microcell'] == assessing_mic]\n",
    "            if not current_coords.empty:\n",
    "                lat1 = current_coords['latitude'].values[0]\n",
    "                lon1 = current_coords['longitude'].values[0]\n",
    "\n",
    "                df_microcell_part = df_microcell.copy()\n",
    "                df_microcell_part.loc[:, 'currect_microcell'] = assessing_mic\n",
    "                \n",
    "                # df_microcell['currect_microcell'] = assessing_mic\n",
    "                replicated_parts = [df_microcell_part]\n",
    "                for provider_id in df_microcell['providerid'].unique():\n",
    "                    candidate_microcells = []\n",
    "                    provider_remote = remote_data[remote_data['providerid'] == provider_id]\n",
    "                    for _, row in microcell_coords.iterrows():\n",
    "                        if row['gen_microcell'] == assessing_mic:\n",
    "                            continue\n",
    "                        if (provider_remote['gen_microcell'] == row['gen_microcell']).any():\n",
    "                            dist = _haversine_km(lat1, lon1, row['latitude'], row['longitude'])\n",
    "                            candidate_microcells.append((row['gen_microcell'], dist))\n",
    "\n",
    "                    if candidate_microcells:\n",
    "                        candidate_microcells.sort(key=lambda x: x[1])\n",
    "                        nearby_microcells = [m for m, _ in candidate_microcells[:10]]\n",
    "                    else:\n",
    "                        nearby_microcells = []\n",
    "\n",
    "                    if nearby_microcells:\n",
    "                        remote_mask = (\n",
    "                            (remote_data['providerid'] == provider_id) &\n",
    "                            (remote_data['gen_microcell'].isin(nearby_microcells)) &\n",
    "                            (remote_data['gen_microcell'] != assessing_mic)\n",
    "                        )\n",
    "                        df_remote = remote_data.loc[remote_mask].copy()\n",
    "                        if not df_remote.empty:\n",
    "                            df_remote = df_remote.drop_duplicates(subset='serviceid')\n",
    "                            df_remote['origin'] = 'R'\n",
    "                            df_remote['currect_microcell'] = assessing_mic\n",
    "                            replicated_parts.append(df_remote)\n",
    "\n",
    "                df_microcell_replicated = pd.concat(replicated_parts, ignore_index=True)\n",
    "            else:\n",
    "                df_microcell_replicated = df_microcell.copy()\n",
    "\n",
    "            #df_microcell_replicated contain local generated data, and data gathered from 10 remte microcells per provider now lets assume the lacal super provider is also tamper with the data. \n",
    "\n",
    "            spa_tampered_df = tampering.spa_tampering(\n",
    "                        df_microcell_replicated,\n",
    "                        type=tampering_type\n",
    "                    )\n",
    "            \n",
    "            # if assessing_mic=='M102':\n",
    "            #     print(spa_tampered_df['currect_microcell'].unique())\n",
    "\n",
    "            #     spa_tampered_df.to_csv(\"test.csv\",index=False)\n",
    "            #     bma_tampered_df.to_csv(\"test2.csv\",index=False)\n",
    "\n",
    "                \n",
    "\n",
    "            #     print(df_microcell_replicated['gen_microcell'].unique())\n",
    "            # else:\n",
    "            #     continue\n",
    "\n",
    "\n",
    "            \n",
    "            data_service.set_local_data(spa_tampered_df.copy())\n",
    "            data_service.set_remote_data(bma_tampered_df.copy())\n",
    "\n",
    "            # For each provider, evaluate trust score\n",
    "            for provider in df_microcell['providerid'].unique():\n",
    "                trust_score, df = timf.trust_assessment(provider, assessing_mic)\n",
    "\n",
    "                if not df.empty and 'true_label' in df.columns and 'label' in df.columns:\n",
    "                    y_true_bin = (df['true_label'] == 'T').astype(int)\n",
    "                    \n",
    "                    # Compute binary prediction metrics (hard labels)\n",
    "                    y_pred_bin = (df['label'] == 'T').astype(int)\n",
    "                    accuracy = accuracy_score(y_true_bin, y_pred_bin)\n",
    "                    precision = precision_score(y_true_bin, y_pred_bin, zero_division=0)\n",
    "                    recall = recall_score(y_true_bin, y_pred_bin, zero_division=0)\n",
    "\n",
    "                    # Compute AUC using the trust score (soft prediction)\n",
    "                    try:\n",
    "                        auc = roc_auc_score(y_true_bin, trust_score)\n",
    "                    except ValueError:\n",
    "                        auc = None  # Handle single-class edge case\n",
    "\n",
    "                    results_metrics.append({\n",
    "                        'tampering_type': tampering_type,\n",
    "                        'tampering_percentage': tampering_percentage,\n",
    "                        'microcell': assessing_mic,\n",
    "                        'provider_id': provider,\n",
    "                        'method': 'TDA',\n",
    "                        'accuracy': accuracy,\n",
    "                        'precision': precision,\n",
    "                        'recall': recall,\n",
    "                        'auc': auc\n",
    "                    })\n",
    "                else:\n",
    "                    print(\"issue with df for metrics calculation\")\n",
    "\n",
    "                # ---- Baseline methods ----\n",
    "                baseline_results = timf.trust_assessment_baseline(provider, assessing_mic)\n",
    "                for method_name, df_baseline in baseline_results.items():\n",
    "                    if not df_baseline.empty and 'true_label' in df_baseline.columns and 'label' in df_baseline.columns:\n",
    "                        y_true_bin = (df_baseline['true_label'] == 'T').astype(int)\n",
    "                        y_pred_bin = (df_baseline['label'] == 'T').astype(int)\n",
    "                        accuracy = accuracy_score(y_true_bin, y_pred_bin)\n",
    "                        precision = precision_score(y_true_bin, y_pred_bin, zero_division=0)\n",
    "                        recall = recall_score(y_true_bin, y_pred_bin, zero_division=0)\n",
    "\n",
    "                        # For baseline methods, if trust score is available, use it; otherwise use label for AUC\n",
    "                        try:\n",
    "                            if 'trust_score' in df_baseline.columns:\n",
    "                                auc = roc_auc_score(y_true_bin, df_baseline['trust_score'])\n",
    "                            else:\n",
    "                                auc = roc_auc_score(y_true_bin, y_pred_bin)\n",
    "                        except ValueError:\n",
    "                            auc = None\n",
    "\n",
    "                        results_metrics.append({\n",
    "                            'tampering_type': tampering_type,\n",
    "                            'tampering_percentage': tampering_percentage,\n",
    "                            'microcell': assessing_mic,\n",
    "                            'provider_id': provider,\n",
    "                            'method': method_name,\n",
    "                            'accuracy': accuracy,\n",
    "                            'precision': precision,\n",
    "                            'recall': recall,\n",
    "                            'auc': auc\n",
    "                        })\n",
    "                    else:\n",
    "                        print(\"issue with df for metrics calculation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab833d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e731f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_metrics = pd.DataFrame(results_metrics)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(\n",
    "    data=df_metrics,\n",
    "    x='tampering_percentage',\n",
    "    y='auc',\n",
    "    hue='method',            # Different methods (TDA, baselines)\n",
    "    style='tampering_type',  # Optional: different line style per tampering type\n",
    "    markers=True,\n",
    "    dashes=False\n",
    ")\n",
    "plt.title(\"AUC vs Tampering Percentage\")\n",
    "plt.xlabel(\"Tampering Percentage (%)\")\n",
    "plt.ylabel(\"AUC\")\n",
    "plt.ylim(0, 1)  # AUC is always between 0 and 1\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086f7432",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827c9f92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbc7754",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f418dcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9afcfcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f82d74e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bd5cd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4662fb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961ffcad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377fb265",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510e6f16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f7acb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = pd.DataFrame(results_metrics).dropna(subset=['accuracy','precision','recall'])\n",
    "summary = metrics_df.groupby(['tampering_type','tampering_percentage'], as_index=False)[['accuracy','precision','recall']].mean()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5), sharex=True)\n",
    "metric_names = ['accuracy', 'precision', 'recall']\n",
    "\n",
    "for idx, metric in enumerate(metric_names):\n",
    "    ax = axes[idx]\n",
    "    for t_type in tampering_types:\n",
    "        subset = summary[summary['tampering_type'] == t_type]\n",
    "        ax.plot(subset['tampering_percentage'], subset[metric], marker='o', label=t_type)\n",
    "    ax.set_title(metric.capitalize())\n",
    "    ax.set_xlabel('Tampering Percentage')\n",
    "    ax.set_ylabel(metric.capitalize())\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "axes[0].legend(title='Type')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0f7dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Setup for Paper Quality\n",
    "plt.rcParams.update({\n",
    "    'font.size': 12,\n",
    "    'font.family': 'serif',\n",
    "    'axes.titlesize': 14,\n",
    "    'axes.labelsize': 12,\n",
    "    'xtick.labelsize': 10,\n",
    "    'ytick.labelsize': 10,\n",
    "    'legend.fontsize': 11,\n",
    "    'figure.dpi': 300\n",
    "})\n",
    "\n",
    "# Use a colorblind-friendly style\n",
    "plt.style.use('tableau-colorblind10')\n",
    "\n",
    "# 2. Data Preparation\n",
    "metrics_df = pd.DataFrame(results_metrics).dropna(subset=['accuracy','precision','recall'])\n",
    "summary = metrics_df.groupby(['tampering_type','tampering_percentage'], as_index=False)[['accuracy','precision','recall']].mean()\n",
    "tampering_types = summary['tampering_type'].unique()\n",
    "\n",
    "# 3. Define distinctive markers and line styles\n",
    "markers = ['o', 's', '^', 'D', 'v', 'p', 'X']\n",
    "linestyles = ['-', '--', '-.', ':', (0, (3, 5, 1, 5)), (0, (5, 10))]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5), sharex=True)\n",
    "metric_names = ['accuracy', 'precision', 'recall']\n",
    "\n",
    "for idx, metric in enumerate(metric_names):\n",
    "    ax = axes[idx]\n",
    "    for i, t_type in enumerate(tampering_types):\n",
    "        subset = summary[summary['tampering_type'] == t_type]\n",
    "        ax.plot(\n",
    "            subset['tampering_percentage'], \n",
    "            subset[metric], \n",
    "            label=t_type,\n",
    "            marker=markers[i % len(markers)], \n",
    "            linestyle=linestyles[i % len(linestyles)],\n",
    "            linewidth=2,\n",
    "            markersize=7,\n",
    "            alpha=0.85\n",
    "        )\n",
    "    \n",
    "    # Aesthetic adjustments\n",
    "    ax.set_title(metric.capitalize(), fontweight='bold', pad=10)\n",
    "    ax.set_xlabel('Tampering Percentage')\n",
    "    if idx == 0:\n",
    "        ax.set_ylabel('Score')\n",
    "    \n",
    "    ax.set_ylim(0, 1.05)\n",
    "    ax.grid(True, linestyle='--', alpha=0.5)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "\n",
    "# 4. Centralized Legend below the plots\n",
    "handles, labels = axes[0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='lower center', bbox_to_anchor=(0.5, -0.05),\n",
    "           ncol=len(tampering_types), frameon=False, title='Tampering Method')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.05, 1, 1])\n",
    "plt.savefig('integrity_metrics_results.pdf', bbox_inches='tight') # PDF is best for LaTeX\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c75f28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022efc86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66ce233",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f590b80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d725a6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('experiment_1_original_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68720978",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3739b401",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0647f9ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa79cf49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a339c590",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e701e4f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae6b4e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4985d7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Evaluation framework for Trust Information Management Framework (TIMF).\n",
    "\n",
    "This module provides controlled experiments to evaluate TIMF under various\n",
    "tampering scenarios in a crowdsourced IoT environment.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Evaluations:\n",
    "    def __init__(self):\n",
    "        self.evaluation_data = EvaluationData()\n",
    "        self.tampering = Tampering()\n",
    "        \n",
    "        # Initialize DataService and TIMF properly\n",
    "        self.data_service = DataService()\n",
    "        self.timf = TIMF(self.data_service)\n",
    "\n",
    "    def setup_experments(self):\n",
    "      \n",
    "        self.data = self.evaluation_data.get_data()\n",
    "        print(\"Experment setup is complete.\")\n",
    "        \n",
    "    def experiment_1(self, tampering_percentages=None, tampering_types=None):\n",
    "        \"\"\"\n",
    "        Run experiment 1: Evaluate TIMF under controlled tampering scenarios.\n",
    "        \n",
    "        This experiment:\n",
    "        - Applies synthetic tampering to data\n",
    "        - Varies tampering percentage and type\n",
    "        - Runs trust assessment for each provider in each microcell\n",
    "        - Records results\n",
    "        \n",
    "        Args:\n",
    "            tampering_percentages: List of percentages to test (default: 10 to 90 in steps of 10)\n",
    "            tampering_types: List of tampering types [\"N\", \"K\", \"S\"] (default: all)\n",
    "        \n",
    "        Returns:\n",
    "            results: Dictionary with experiment results\n",
    "        \"\"\"\n",
    "        if tampering_percentages is None:\n",
    "            tampering_percentages = list(range(10, 100, 10))\n",
    "        if tampering_types is None:\n",
    "            tampering_types = [\"N\", \"K\", \"S\"]  # Naive, Knowledgeable, Sophisticated\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        \n",
    "        untampered_data = self.data.copy()\n",
    "        \n",
    "        for tampering_type in tampering_types:\n",
    "            for tampering_percentage in tampering_percentages:\n",
    "                print(f\"Experiment: Tampering Type={tampering_type}, Percentage={tampering_percentage}%\")\n",
    "        \n",
    "             \n",
    "\n",
    "                # For each provider in each microcell, run trust assessment\n",
    "                for microcell in self.data['microcell'].unique():\n",
    "                    df_microcell = self.data[self.data['microcell'] == microcell]\n",
    "                    # print(f\"\\nMicrocell: {microcell}\")\n",
    "\n",
    "                    # --- Per-microcell tampering setup ---\n",
    "                    local_key = str(microcell)\n",
    "\n",
    "              \n",
    "                    spa_tampered_dict = self.tampering.spa_tampering(\n",
    "                        self.data[self.data['microcell'] == microcell],\n",
    "                        sp_percent=100,              # always tamper the local microcell\n",
    "                        type=tampering_type\n",
    "                    )\n",
    "\n",
    "                    bma_tampered_df = self.tampering.bma_tampering(\n",
    "                        self.data[self.data['microcell'] != microcell],\n",
    "                        tampering_percentage,\n",
    "                        tampering_type\n",
    "                    )\n",
    "                   \n",
    "               \n",
    "                    # Set tampered and untampered data directly in data service\n",
    "                    self.data_service.set_tampered_data(spa_tampered_dict,bma_tampered_df)\n",
    "                \n",
    "\n",
    "                    # --- Trust assessment for all providers in this local microcell ---\n",
    "                    for provider in df_microcell['providerid'].unique():\n",
    "                       \n",
    "                        trust_score, df = self.timf.trust_assessment(provider, microcell)\n",
    "                        \n",
    "                        # Calculate accuracy, precision, and recall by comparing true_label and label\n",
    "                        \n",
    "                        metrics = self._calculate_metrics(df['true_label'], df['label'])\n",
    "                            \n",
    "                        result = {\n",
    "                                'tampering_type': tampering_type,\n",
    "                                'tampering_percentage': tampering_percentage,\n",
    "                                'microcell': microcell,\n",
    "                                'provider_id': provider,\n",
    "                                'trust_score': trust_score,\n",
    "                                'accuracy': metrics['accuracy'],\n",
    "                                'precision': metrics['precision'],\n",
    "                                'recall': metrics['recall']\n",
    "                            }\n",
    "                        results.append(result)\n",
    "                            \n",
    "                        # print(f\"  Provider: {provider} -> Trust score: {trust_score:.4f}, \"\n",
    "                        #           f\"Accuracy: {metrics['accuracy']:.4f}, \"\n",
    "                        #           f\"Precision: {metrics['precision']:.4f}, \"\n",
    "                        #           f\"Recall: {metrics['recall']:.4f}\")\n",
    "                        \n",
    "        print(results)\n",
    "        # Convert results to DataFrame for analysis\n",
    "        results_df = pd.DataFrame(results)\n",
    "        return results_df\n",
    "    \n",
    "    def _calculate_metrics(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Calculate accuracy, precision, and recall metrics.\n",
    "        \n",
    "        Args:\n",
    "            y_true: Series with true labels ('C' for Correct, 'T' for Tampered)\n",
    "            y_pred: Series with predicted labels ('C' for Correct, 'T' for Tampered)\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with accuracy, precision, and recall\n",
    "        \"\"\"\n",
    "        # Convert to binary: 'T' = 1 (positive), 'C' = 0 (negative)\n",
    "        y_true_binary = (y_true == 'T').astype(int)\n",
    "        y_pred_binary = (y_pred == 'T').astype(int)\n",
    "        \n",
    "        # Calculate True Positives, False Positives, True Negatives, False Negatives\n",
    "        tp = ((y_true_binary == 1) & (y_pred_binary == 1)).sum()\n",
    "        fp = ((y_true_binary == 0) & (y_pred_binary == 1)).sum()\n",
    "        tn = ((y_true_binary == 0) & (y_pred_binary == 0)).sum()\n",
    "        fn = ((y_true_binary == 1) & (y_pred_binary == 0)).sum()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        total = len(y_true)\n",
    "        accuracy = (tp + tn) / total if total > 0 else 0.0\n",
    "        \n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        \n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        \n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def _dictionary_to_merged_df(self,dic):\n",
    "        temp = pd.concat(dic.values(), ignore_index=True)\n",
    "        temp.reset_index(drop=True, inplace=True)\n",
    "        return temp\n",
    "\n",
    "    def _dataframe_divide_to_microcell_dictionary(self, df):\n",
    "        \"\"\"\n",
    "        Divide a DataFrame into a dictionary keyed by microcell.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame with 'microcell' column\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with microcell as key, DataFrame as value\n",
    "        \"\"\"\n",
    "        temp_dictionary = {}\n",
    "        unique_keys = df.microcell.unique()\n",
    "        for microcell in unique_keys:\n",
    "            temp_dictionary[str(microcell)] = df[df.microcell == microcell].copy()\n",
    "        return temp_dictionary\n",
    "      \n",
    "\n",
    "\n",
    "\n",
    "        # Run experiment 1\n",
    "       \n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "# dfin = general.open_file_csv('data_alg_16000.csv')\n",
    "# dfin = general.slice_df(dfin,['serviceid','providerid','microcell','timestamp','speed','latency','bandwidth','coverage','reliability','security','currect_microcell'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881929db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paper1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
